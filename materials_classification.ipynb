{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soft Robotics Material Classification using Tensile Data\n",
    "Rui Du, 260914334"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background and Motivation\n",
    "\n",
    "This objective of this experiment is to classify a soft material under tension when given an arbitrary window of tensile test results. The data used in this experiment consists of tensile test data for 19 different materials. For each material, the true stress, true strain, engineering stress, and engineering strain are collected over a time period at regular increments. If we take a small, continuous time window of the test results from a given material, can we predict which material is currently under tension using a logistic regression classifier? Additionally, how would a perceptron compare?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from preprocess import get_all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing\n",
    "\n",
    "The first challenge of this problem lies in how to structure the data. Since we want to do classification over continuous time series data, we want to keep the continuity in a given data point, ie. if each data point consists of 3 measurements, they have to be time-consecutive. However, we still want to shuffle the training data as well as have the measurements in the test data be completely unseen by the model during training.\n",
    "\n",
    "To meet the above requirements, the following steps were done during preprocessing for each material:\n",
    "- Remove (`test_split` * number of measurements) consecutive measurements at a random position in the data. This is to ensure that the test data will not always be from the beginning/middle/end section of the measurements.\n",
    "- For both the remaining train data and the extracted test data, overlapping windows of `interval` length were constructed as the inputs to the model, and labeled with the corresponding material under tension\n",
    "- Each data point was then flattened into a one dimensional array\n",
    "\n",
    "Finally, an additional split of the training data is performed using scikit-learn's `train_test_split` to shuffle training data and make a validation split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using custom function to preprocess data and making sure all time points in test data are \"unseen\"\n",
    "x_train, y_train, x_test, y_test, labels = get_all_data(test_split=0.1, interval=3, verbose=False)\n",
    "\n",
    "# shuffle data into train and validation data\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1, random_state=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11.8         1.29721     1.66299     2.6590736   0.45448389 11.9\n",
      "   1.30341     1.69404     2.68183033  0.46010811 12.          1.30956\n",
      "   1.72569     2.70454335  0.46583069]\n",
      " [25.1         1.90279     1.28280405  5.70454     0.19133    25.2\n",
      "   1.90617     1.29704742  5.72727     0.1928     25.3         1.90954\n",
      "   1.31259048  5.75        0.19446   ]\n",
      " [40.239       2.74230667 37.55404878 14.52272667  2.41929333 40.24\n",
      "   2.74377    37.70412804 14.54545333  2.42541333 40.241       2.74523\n",
      "  37.84646556 14.56818     2.43101333]]\n",
      "[3 7 5]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[:3])\n",
    "print(y_train[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now train a Logistic Regression model on the training data, using different regularization methods and optimizers. For each experiment, the `max_iter` default value of 100 provided by scikit-learn was not sufficient to obtain a decent validation accuracy, so they were increased until at least one method had more than 90% validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "penalties = ['l2', None] # default=l2\n",
    "solvers = ['lbfgs', 'sag'] # default=lbfgs\n",
    "# max_iters = [100, 1e4, 1e6, 1e8] # default=100\n",
    "\n",
    "# model = LogisticRegression(penalty=penalty, solver=solver, max_iter=max_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy of 0.7116402116402116 with regularization: l2\n",
      "Validation Accuracy of 0.9470899470899471 with regularization: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ruidu/Documents/McGill/F2023/MECH 501/Soft-Robotics-Materials/.venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of f AND g EVALUATIONS EXCEEDS LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "for penalty in penalties:\n",
    "    model = LogisticRegression(penalty=penalty, random_state=11, max_iter=int(1e10)).fit(x_train, y_train)\n",
    "    val_acc = model.score(x_val, y_val)\n",
    "    \n",
    "    print(f'Validation Accuracy of {val_acc} with regularization: {penalty}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the model results in better validation accuracy without L2 regularization, which could indicate that L2 regularization is causing the data to underfit in this case.\n",
    "\n",
    "> Validation Accuracy w/ L2 Regularization: 0.71\n",
    "\n",
    "> Validation Accuracy w/o Regularization: 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy of 0.9323607427055703 with solver lbfgs\n",
      "Validation Accuracy of 0.7201591511936339 with solver sag\n"
     ]
    }
   ],
   "source": [
    "for solver in solvers:\n",
    "    model = LogisticRegression(penalty=None, random_state=11, max_iter=int(1e4), solver=solver).fit(x_train, y_train)\n",
    "    val_acc = model.score(x_val, y_val)\n",
    "\n",
    "    print(f'Validation Accuracy of {val_acc} with solver {solver}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above results, we can obserive that the [`lbfgs`](https://users.iems.northwestern.edu/~nocedal/lbfgsb.html) solver gives a better accuracy than the [`sag`](https://inria.hal.science/hal-00860051/document) one.\n",
    "\n",
    "> Validation Accuracy w/ `lbfgs`: 0.93\n",
    "\n",
    "> Validation Accuracy w/ `sag`: 0.72"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = ['log_loss', 'perceptron'] # logisitc regression with SGD vs Perceptron with SGD\n",
    "penalties = ['l2', 'l1', None] # default=l2\n",
    "reg_coeffs = [1e-4, 2e-4, 3e-4, 4e-5, 5e-5]\n",
    "max_iters = [100, 1e4, 1e5, 1e6]\n",
    "learning_rate_types = ['constant', 'optimal', 'adaptive']\n",
    "initial_lrs = [1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2]\n",
    "\n",
    "#model = SGDClassifier(loss=classifier, penalty=penalty, alpha=reg_coeff, max_iter=max_iter, random_state=11, learning_rate=learning_rate_type, eta0=initial_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy of 0.35278514588859416 with Classifier log_loss\n",
      "Validation Accuracy of 0.33554376657824936 with Classifier perceptron\n"
     ]
    }
   ],
   "source": [
    "for classifier in classifiers:\n",
    "    model = SGDClassifier(loss=classifier, max_iter=int(1e8), random_state=11).fit(x_train, y_train)\n",
    "    val_acc = model.score(x_val, y_val)\n",
    "\n",
    "    print(f'Validation Accuracy of {val_acc} with Classifier {classifier}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy of 0.35278514588859416 with Regularization l2\n",
      "Validation Accuracy of 0.4005305039787798 with Regularization l1\n",
      "Validation Accuracy of 0.3713527851458886 with Regularization None\n"
     ]
    }
   ],
   "source": [
    "for penalty in penalties:\n",
    "    model = SGDClassifier(loss='log_loss', random_state=11, penalty=penalty).fit(x_train, y_train)\n",
    "    val_acc = model.score(x_val, y_val)\n",
    "\n",
    "    print(f'Validation Accuracy of {val_acc} with Regularization {penalty}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy of 0.4005305039787798 with Regularization Coefficient 0.0001\n",
      "Validation Accuracy of 0.34350132625994695 with Regularization Coefficient 0.0002\n",
      "Validation Accuracy of 0.35013262599469497 with Regularization Coefficient 0.0003\n",
      "Validation Accuracy of 0.40185676392572944 with Regularization Coefficient 4e-05\n",
      "Validation Accuracy of 0.35013262599469497 with Regularization Coefficient 5e-05\n"
     ]
    }
   ],
   "source": [
    "for reg_coeff in reg_coeffs:\n",
    "    model = SGDClassifier(loss='log_loss', random_state=11, penalty='l1', alpha=reg_coeff).fit(x_train, y_train)\n",
    "    val_acc = model.score(x_val, y_val)\n",
    "\n",
    "    print(f'Validation Accuracy of {val_acc} with Regularization Coefficient {reg_coeff}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy of 0.3594164456233422 with a constant Learning Rate\n",
      "Validation Accuracy of 0.40185676392572944 with a optimal Learning Rate\n",
      "Validation Accuracy of 0.34880636604774534 with a adaptive Learning Rate\n"
     ]
    }
   ],
   "source": [
    "for learning_rate_type in learning_rate_types:\n",
    "    model = SGDClassifier(loss='log_loss', random_state=11, penalty='l1', alpha=4e-5, max_iter=int(1e4), learning_rate=learning_rate_type, eta0=1e-4).fit(x_train, y_train)\n",
    "    val_acc = model.score(x_val, y_val)\n",
    "\n",
    "    print(f'Validation Accuracy of {val_acc} with a {learning_rate_type} Learning Rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy of 0.40185676392572944 with Initial Learning Rate 1e-07\n",
      "Validation Accuracy of 0.40185676392572944 with Initial Learning Rate 1e-06\n",
      "Validation Accuracy of 0.40185676392572944 with Initial Learning Rate 1e-05\n",
      "Validation Accuracy of 0.40185676392572944 with Initial Learning Rate 0.0001\n",
      "Validation Accuracy of 0.40185676392572944 with Initial Learning Rate 0.001\n",
      "Validation Accuracy of 0.40185676392572944 with Initial Learning Rate 0.01\n"
     ]
    }
   ],
   "source": [
    "for initial_lr in initial_lrs:\n",
    "    model = SGDClassifier(loss='log_loss', random_state=11, penalty='l1', alpha=4e-5, max_iter=int(1e4), learning_rate='optimal', eta0=initial_lr).fit(x_train, y_train)\n",
    "    val_acc = model.score(x_val, y_val)\n",
    "\n",
    "    print(f'Validation Accuracy of {val_acc} with Initial Learning Rate {initial_lr}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9511278195488722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of f AND g EVALUATIONS EXCEEDS LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(penalty=None, random_state=11, max_iter=int(1e8), solver='lbfgs').fit(x_train, y_train)\n",
    "test_acc = model.score(x_test, y_test)\n",
    "print(f'Test Accuracy: {test_acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results and Conclusion\n",
    "\n",
    "From the above experiments, we can observe that the best performing model is a logistic regression model using `lbfgs` optimizer and no regularization. This could be due to regularization causing underfitting in the model.\n",
    "\n",
    "Test accuracy was evaluated on the best performing model to be 95%, which indicates that the model generalizes well to unseen data.\n",
    "\n",
    "We can thus conclude that logistic regression allows us to classify a material based on a consecutive window of tensile measurements\n",
    "\n",
    "SGD optimizer was used for logistic regression as well, but offered significant decrease in accuracy. Several parameters such as learning rate type (adaptive, constant, optimal), initial learning rate, regularization type and regularization coefficient were evaluated but validation accuracy hovered around 40%.\n",
    "\n",
    "While SGD offered worse results, it had a much quicker training time (up to 10s), whereas `lbfgs` took up to 10 minutes to train depending on the maximum number of iterations. It should also be noted that `lbfgs` sometimes terminated because it reached the maximum number of iterations without converging, yet still offered 94.71% validation accuracy and 95.11 test accuracy.\n",
    "\n",
    "Another notable finding is that a window as small as 3 consecutive measurements can already allow high accuracy when categorizing the material under tension."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
